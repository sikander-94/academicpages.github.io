---
title: "Simple and optimal high-probability bounds for strongly-convex stochastic gradient descent"
collection: publications
permalink: /publication/2019-09-04-sgd
venue: "arXiv"
date: 2019-09-01
year: 2019
with: '<a href="https://www.cs.ubc.ca/~nickhar/">Nick Harvey</a> and <a href="https://www.cs.ubc.ca/~cvliaw">Chris Liaw</a>'
links: '<a href="https://arxiv.org/abs/1909.00843">arXiv</a>'
slides: '<a href="https://sikander-randhawa.github.io/files/non-uni-avg-talk-ubc.pdf">slides</a>'
---

---

We consider stochastic gradient descent algorithms for minimizing a non-smooth, strongly-convex function. Several forms of this algorithm, including suffix averaging, are known to achieve the optimal $O(1/T)$ convergence rate in expectation. We consider a simple, non-uniform averaging strategy of Lacoste-Julien et al. (2011) and prove that it achieves the optimal $O(1/T)$ convergence rate with high probability. Our proof uses a recently developed generalization of Freedman's inequality. Finally, we compare several of these algorithms experimentally and show that this non-uniform averaging strategy outperforms many standard techniques, and with smaller variance.

View on [arXiv](https://arxiv.org/abs/1812.05217) or download [here](http://sikander-randhawa.github.io/files/sgd-non-uni.pdf). See slides [here](https://sikander-randhawa.github.io/talks/sgd-non-uni-talk-ubc).

